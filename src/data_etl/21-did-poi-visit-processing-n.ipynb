{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DiD visits data processing\n",
    "Organize data ready for DiD modeling."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59c8ef40326ec983"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\nine-euro-ticket-de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd D:\\nine-euro-ticket-de"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:34.370829600Z",
     "start_time": "2024-10-14T19:56:34.258910900Z"
    }
   },
   "id": "d30331718890931b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load libs\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import workers\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from p_tqdm import p_map\n",
    "from statsmodels.stats.weightstats import DescrStatsW"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:36.921251100Z",
     "start_time": "2024-10-14T19:56:34.356653600Z"
    }
   },
   "id": "2ad38e85cd0df174"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Data location\n",
    "user = workers.keys_manager['database']['user']\n",
    "password = workers.keys_manager['database']['password']\n",
    "port = workers.keys_manager['database']['port']\n",
    "db_name = workers.keys_manager['database']['name']\n",
    "engine = sqlalchemy.create_engine(f'postgresql://{user}:{password}@localhost:{port}/{db_name}?gssencmode=disable')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:37.111256300Z",
     "start_time": "2024-10-14T19:56:36.923252900Z"
    }
   },
   "id": "cdaa2a28d6a91a23"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'dbs/poi2visits_day_did/stops_0.parquet'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('dbs/poi2visits_day_did/')\n",
    "paths2stops = {int(x.split('_')[-1].split('.')[0]): os.path.join(data_folder, x)\\\n",
    "               for x in list(os.walk(data_folder))[0][2]}\n",
    "paths2stops_list = list(paths2stops.values())\n",
    "paths2stops_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:37.257312Z",
     "start_time": "2024-10-14T19:56:37.114256300Z"
    }
   },
   "id": "c8ddc11defbca429"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_cat = pd.read_excel('dbs/poi/categories.xlsx').rename(columns={'category': 'theme', 'subcategory': 'label'})\n",
    "label_list = df_cat['label'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:38.072118200Z",
     "start_time": "2024-10-14T19:56:37.256193300Z"
    }
   },
   "id": "3cb5207f3a75853a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def ice(ai=None, bi=None, popi=None, share_a=0.25, share_b=0.25):\n",
    "    oi = popi - ai - bi\n",
    "    share_o = 1 - share_a - share_b\n",
    "    return (ai / share_a - bi / share_b) / (ai / share_a + bi / share_b + oi / share_o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:38.208894600Z",
     "start_time": "2024-10-14T19:56:38.074186Z"
    }
   },
   "id": "36ef6cc75081f20e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "826e8f289b1efcfa"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145267 412767\n"
     ]
    }
   ],
   "source": [
    "df_osm = pd.read_parquet('dbs/places_matching/matched_places_wt.parquet')\n",
    "df_osm_a = pd.read_parquet('dbs/places_matching/places_co_ys.parquet')\n",
    "print(df_osm.osm_id.nunique(), df_osm_a.osm_id.nunique())\n",
    "osm_ids = list(df_osm_a['osm_id'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:56:59.348691700Z",
     "start_time": "2024-10-14T19:56:58.440702700Z"
    }
   },
   "id": "cb42290ee5bfeac0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_label_visits(lb=None, paths2stops_list=None):\n",
    "    df_t_list = []\n",
    "    for i in tqdm(paths2stops_list, desc=f'Getting {lb}'):\n",
    "        tp = pd.read_parquet(i)\n",
    "        if lb is not None:\n",
    "            tp = tp.loc[tp['label'] == lb, :]\n",
    "        df_t_list.append(tp)\n",
    "    df_t = pd.concat(df_t_list)\n",
    "    return df_t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:23:02.961931900Z",
     "start_time": "2024-10-14T19:23:02.803459700Z"
    }
   },
   "id": "dc0367872fd258f5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting None: 100%|██████████| 300/300 [01:13<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "df_t = load_label_visits(lb=None, paths2stops_list=paths2stops_list)\n",
    "df_t = df_t.loc[df_t.osm_id.isin(osm_ids), :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:28:13.569261300Z",
     "start_time": "2024-10-14T19:26:04.174687600Z"
    }
   },
   "id": "5a3ed9eaddc1c322"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing by label: 100%|██████████| 52/52 [07:12<00:00,  8.32s/it]\n"
     ]
    }
   ],
   "source": [
    "for lb in tqdm(label_list, desc='Writing by label'):\n",
    "    df_t.loc[df_t.label==lb,:].to_parquet(f'dbs/temp/{lb}.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:35:26.250884Z",
     "start_time": "2024-10-14T19:28:13.600147100Z"
    }
   },
   "id": "1ac27e7a99e2973d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Calculate visitation attributes - daily DiD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57472d458ca97528"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def visit_patterns(data):\n",
    "    data.loc[:, 'date'] = data.loc[:, 'date'].astype(str)\n",
    "    metrics_dict = dict()\n",
    "    # osm_id info\n",
    "    for var in ('osm_id', 'date', 'year', 'month', 'weekday', 'theme', 'label', 'precipitation', 'pt_station_num'):\n",
    "        metrics_dict[var] = data[var].values[0]\n",
    "    # Visits\n",
    "    metrics_dict['num_visits_wt'] = data['wt_p'].sum()\n",
    "    metrics_dict['num_unique_device'] = data.device_aid.nunique()\n",
    "    # Duration\n",
    "    metrics_dict['dur_total_wt'] = sum(data['dur'] * data['wt_p'])   # min\n",
    "\n",
    "    # Distance from home\n",
    "    ## Weighted percentiles\n",
    "    d, wt = data.loc[data['d_h'] > 0, 'd_h'], data.loc[data['d_h'] > 0, 'wt_p']\n",
    "    wdf = DescrStatsW(d, weights=wt, ddof=1)\n",
    "    sts = wdf.quantile([0.25, 0.5, 0.75])\n",
    "    bds = sts.values\n",
    "    metrics_dict['d_h25_wt'], metrics_dict['d_h50_wt'], metrics_dict['d_h75_wt'] = bds[0], bds[1], bds[2]\n",
    "    \n",
    "    # Segregation metric\n",
    "    pop = np.sum(data.wt_p)\n",
    "    a = np.sum(data.loc[data.grdi_grp=='H', 'wt_p'])\n",
    "    b = np.sum(data.loc[data.grdi_grp=='L', 'wt_p'])\n",
    "    metrics_dict['ice'] = ice(ai=a, bi=b, popi=pop, share_a=0.25, share_b=0.25)\n",
    "    metrics_dict['H'], metrics_dict['L'], metrics_dict['M'] = a/pop, b/pop, (pop-a-b)/pop\n",
    "    \n",
    "    ## weighted average\n",
    "    d_lg = d.apply(lambda x: np.log10(x))\n",
    "    metrics_dict['d_ha_wt'] = 10**np.average(d_lg, weights=wt)\n",
    "    return pd.Series(metrics_dict)  # pd.DataFrame(metrics_dict, index=[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:57:19.200475200Z",
     "start_time": "2024-10-14T19:57:19.097969Z"
    }
   },
   "id": "dd03dce1bccea60d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive and services\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m df_t \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbs/temp/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlb\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Group by the necessary columns and split into separate groups\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m grouped_data \u001B[38;5;241m=\u001B[39m [group \u001B[38;5;28;01mfor\u001B[39;00m _, group \u001B[38;5;129;01min\u001B[39;00m df_t\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mosm_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])]\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Apply the function in parallel using p_map\u001B[39;00m\n\u001B[0;32m     13\u001B[0m df_v \u001B[38;5;241m=\u001B[39m p_map(visit_patterns_parallel, grouped_data, num_cpus\u001B[38;5;241m=\u001B[39mcpu_count())\n",
      "Cell \u001B[1;32mIn[16], line 10\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      8\u001B[0m df_t \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbs/temp/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlb\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Group by the necessary columns and split into separate groups\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m grouped_data \u001B[38;5;241m=\u001B[39m [group \u001B[38;5;28;01mfor\u001B[39;00m _, group \u001B[38;5;129;01min\u001B[39;00m df_t\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mosm_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])]\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Apply the function in parallel using p_map\u001B[39;00m\n\u001B[0;32m     13\u001B[0m df_v \u001B[38;5;241m=\u001B[39m p_map(visit_patterns_parallel, grouped_data, num_cpus\u001B[38;5;241m=\u001B[39mcpu_count())\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\geoenv\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:620\u001B[0m, in \u001B[0;36mBaseGrouper.get_iterator\u001B[1;34m(self, data, axis)\u001B[0m\n\u001B[0;32m    618\u001B[0m splitter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_splitter(data, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m    619\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_keys_seq\n\u001B[1;32m--> 620\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(keys, splitter)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\geoenv\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1160\u001B[0m, in \u001B[0;36mDataSplitter.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1157\u001B[0m starts, ends \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mgenerate_slices(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slabels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mngroups)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m start, end \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(starts, ends):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chop\u001B[49m\u001B[43m(\u001B[49m\u001B[43msdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mslice\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\geoenv\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1186\u001B[0m, in \u001B[0;36mFrameSplitter._chop\u001B[1;34m(self, sdata, slice_obj)\u001B[0m\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chop\u001B[39m(\u001B[38;5;28mself\u001B[39m, sdata: DataFrame, slice_obj: \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m   1181\u001B[0m     \u001B[38;5;66;03m# Fastpath equivalent to:\u001B[39;00m\n\u001B[0;32m   1182\u001B[0m     \u001B[38;5;66;03m# if self.axis == 0:\u001B[39;00m\n\u001B[0;32m   1183\u001B[0m     \u001B[38;5;66;03m#     return sdata.iloc[slice_obj]\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m     \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[0;32m   1185\u001B[0m     \u001B[38;5;66;03m#     return sdata.iloc[:, slice_obj]\u001B[39;00m\n\u001B[1;32m-> 1186\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43msdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_slice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslice_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1187\u001B[0m     df \u001B[38;5;241m=\u001B[39m sdata\u001B[38;5;241m.\u001B[39m_constructor_from_mgr(mgr, axes\u001B[38;5;241m=\u001B[39mmgr\u001B[38;5;241m.\u001B[39maxes)\n\u001B[0;32m   1188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\u001B[38;5;241m.\u001B[39m__finalize__(sdata, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroupby\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32minternals.pyx:871\u001B[0m, in \u001B[0;36mpandas._libs.internals.BlockManager.get_slice\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32minternals.pyx:852\u001B[0m, in \u001B[0;36mpandas._libs.internals.BlockManager._slice_mgr_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32minternals.pyx:704\u001B[0m, in \u001B[0;36mpandas._libs.internals.Block.slice_block_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32minternals.pyx:704\u001B[0m, in \u001B[0;36mpandas._libs.internals.Block.slice_block_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32minternals.pyx:710\u001B[0m, in \u001B[0;36mpandas._libs.internals.Block.slice_block_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\geoenv\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:379\u001B[0m, in \u001B[0;36mDatetimeLikeArrayMixin.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    372\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;124;03mThis getitem defers to the underlying array, which by-definition can\u001B[39;00m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;124;03monly handle list-likes, slices, and integer scalars\u001B[39;00m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    376\u001B[0m \u001B[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001B[39;00m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001B[39;00m\n\u001B[1;32m--> 379\u001B[0m result \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnion[Self, DTScalarOrNaT]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(key))\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mis_scalar(result):\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define the function you want to apply in parallel\n",
    "def visit_patterns_parallel(group):\n",
    "    return visit_patterns(group)\n",
    "loc_number_list = []\n",
    "df_list = []\n",
    "for lb in label_list:\n",
    "    print(lb)\n",
    "    df_t = pd.read_parquet(f'dbs/temp/{lb}.parquet')\n",
    "    # Group by the necessary columns and split into separate groups\n",
    "    grouped_data = [group for _, group in df_t.groupby(['osm_id', 'date_time'])]\n",
    "    \n",
    "    # Apply the function in parallel using p_map\n",
    "    df_v = p_map(visit_patterns_parallel, grouped_data, num_cpus=cpu_count())\n",
    "    \n",
    "    # Concatenate the results back into a DataFrame\n",
    "    df_v = pd.concat(df_v).reset_index(drop=True)\n",
    "    # tqdm.pandas()\n",
    "    # df_v = df_t.groupby(['osm_id', 'date_time']).progress_apply(visit_patterns).reset_index(drop=True)\n",
    "    df_v.to_parquet(f\"dbs/visits_day_did/{lb}.parquet\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T19:46:10.302967600Z",
     "start_time": "2024-10-14T19:44:45.934246Z"
    }
   },
   "id": "424df6eed07aad9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive and services\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcb056cf3f1e4147b4003fc5a19cde4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to process a batch of groups\n",
    "def process_batch(batch):\n",
    "    import pandas as pd\n",
    "    from statsmodels.stats.weightstats import DescrStatsW\n",
    "    import numpy as np\n",
    "    import workers\n",
    "    return pd.concat([visit_patterns(group) for group in batch])\n",
    "\n",
    "for lb in label_list:\n",
    "    print(lb)\n",
    "    df_t = pd.read_parquet(f'dbs/temp/{lb}.parquet')\n",
    "    # Split the DataFrame into groups\n",
    "    grouped_data = [group for _, group in df_t.groupby(['osm_id', 'date_time'])]\n",
    "    \n",
    "    # Determine the batch size based on available CPU cores\n",
    "    num_batches = cpu_count() * 2  # Adjust this multiplier based on your system's capacity\n",
    "    batch_size = int(np.ceil(len(grouped_data) / num_batches))\n",
    "    \n",
    "    # Create batches of groups\n",
    "    batches = [grouped_data[i:i + batch_size] for i in range(0, len(grouped_data), batch_size)]\n",
    "    \n",
    "    # Process each batch in parallel using p_map\n",
    "    df_v_batches = p_map(process_batch, batches, num_cpus=cpu_count())\n",
    "    \n",
    "    # Concatenate all batches into a single DataFrame\n",
    "    df_v = pd.concat(df_v_batches).reset_index(drop=True)\n",
    "    df_v.to_parquet(f\"dbs/visits_day_did/{lb}.parquet\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-14T20:06:59.264832600Z"
    }
   },
   "id": "cbc6c661c86a0dfb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Process hex visitation data for DiD modeling",
   "metadata": {
    "collapsed": false
   },
   "id": "dba8aa27649d2edd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\nine-euro-ticket-de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd D:\\nine-euro-ticket-de"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T18:35:41.537758500Z",
     "start_time": "2024-12-01T18:35:41.449536300Z"
    }
   },
   "id": "314bc5e63b5c39d3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load libs\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import workers\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import h3\n",
    "from statsmodels.stats.weightstats import DescrStatsW"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T18:35:48.197373700Z",
     "start_time": "2024-12-01T18:35:41.767604Z"
    }
   },
   "id": "7db663dd3e4210be"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Data location\n",
    "user = workers.keys_manager['database']['user']\n",
    "password = workers.keys_manager['database']['password']\n",
    "port = workers.keys_manager['database']['port']\n",
    "db_name = workers.keys_manager['database']['name']\n",
    "engine = sqlalchemy.create_engine(f'postgresql://{user}:{password}@localhost:{port}/{db_name}?gssencmode=disable')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T18:35:48.411224400Z",
     "start_time": "2024-12-01T18:35:48.197373700Z"
    }
   },
   "id": "3436f6f0c644a631"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version used by PySpark: 1.8.0_401\n",
      "Web UI: http://C19YUEI.net.chalmers.se:4040\n"
     ]
    }
   ],
   "source": [
    "# Pyspark set up\n",
    "os.environ['JAVA_HOME'] = \"C:/Java/jdk-1.8\"\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "# Set up pyspark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# Create new context\n",
    "spark_conf = SparkConf().setMaster(\"local[18]\").setAppName(\"MobiSeg\")\n",
    "spark_conf.set(\"spark.executor.heartbeatInterval\",\"3600s\")\n",
    "spark_conf.set(\"spark.network.timeout\",\"7200s\")\n",
    "spark_conf.set(\"spark.sql.files.ignoreCorruptFiles\",\"true\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"56g\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "spark_conf.set(\"spark.executor.memory\",\"8g\")\n",
    "spark_conf.set(\"spark.memory.fraction\", \"0.6\")\n",
    "spark_conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "java_version = spark._jvm.System.getProperty(\"java.version\")\n",
    "print(f\"Java version used by PySpark: {java_version}\")\n",
    "print('Web UI:', spark.sparkContext.uiWebUrl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T18:36:04.320234400Z",
     "start_time": "2024-12-01T18:35:48.411224400Z"
    }
   },
   "id": "8e65499bb1929c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. POI visitation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddba72f89018643e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'dbs/combined_hex2visits_day_sg/stops_0.parquet'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('dbs/combined_hex2visits_day_sg/')\n",
    "paths2stops = {int(x.split('_')[-1].split('.')[0]): os.path.join(data_folder, x)\\\n",
    "               for x in list(os.walk(data_folder))[0][2]}\n",
    "paths2stops_list = list(paths2stops.values())\n",
    "paths2stops_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T18:47:35.476697500Z",
     "start_time": "2024-11-30T18:47:35.291420900Z"
    }
   },
   "id": "a44c70533eb5d16c"
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(paths2stops_list[0])\n",
    "df.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2781f61937753124",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Auxiliary data\n",
    "### 2.1 Precipitation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b532dc1e53aeb64e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   station_id      date  precipitation\n0           6  20190501            0.0\n1           6  20190502            3.4\n2           6  20190503            1.6\n3           6  20190504            7.0\n4           6  20190505            0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>station_id</th>\n      <th>date</th>\n      <th>precipitation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>20190501</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>20190502</td>\n      <td>3.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>20190503</td>\n      <td>1.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>20190504</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>20190505</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p = pd.read_sql(\"\"\"SELECT station_id, date, \"RS\" AS precipitation FROM precipitation.daily;\"\"\", con=engine)\n",
    "df_poi2p = pd.read_sql(\"\"\"SELECT osm_id, station_id FROM precipitation.poi_station;\"\"\", con=engine)\n",
    "df_p.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T18:47:44.510927Z",
     "start_time": "2024-11-30T18:47:36.319819400Z"
    }
   },
   "id": "eac0bf2ba7fd79bb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df_p.loc[:, 'date_time'] = pd.to_datetime(df_p['date'].astype(str), format='%Y%m%d')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T18:47:45.379385500Z",
     "start_time": "2024-11-30T18:47:44.510927Z"
    }
   },
   "id": "56e24ac7d73800d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 PT access"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e94f3b9ed56f366"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df_pt = pd.read_sql(\"\"\"SELECT osm_id, pt_station_num FROM public_transport.poi_pt_station;\"\"\", con=engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T18:47:50.269510900Z",
     "start_time": "2024-11-30T18:47:45.363749300Z"
    }
   },
   "id": "4f2269ded1b2e218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 National holiday"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3e71398c97053"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "holidays = [np.datetime64(pd.to_datetime(x, format='%Y%m%d')) \n",
    "            for x in [20190501, 20190530, 20190610, \n",
    "                      20220415, 20220418, 20220501, 20220526, 20220606, \n",
    "                      20220407, 20220410, 20230501, 20230518, 20230529]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T18:47:50.394263500Z",
     "start_time": "2024-11-30T18:47:50.269510900Z"
    }
   },
   "id": "b45440d96a3ee149"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Processing visitation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86c5e5275ccdd8a1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328877 visits on 60887 locations from 7648 devices.\n",
      "After removing holidays, 328877 visits on 60887 locations from 7648 devices.\n",
      "After adding public stops, 314134 visits on 60328 locations from 7646 devices.\n"
     ]
    }
   ],
   "source": [
    "# Time processing\n",
    "df.loc[:, 'date_time'] = pd.to_datetime(df['date'].astype(str), format='%Y-%m-%d')\n",
    "print(f'{len(df)} visits on {df.osm_id.nunique()} locations from {df.device_aid.nunique()} devices.')\n",
    "\n",
    "# Filter out national holidays\n",
    "df.loc[:, 'date_time'] = df.loc[~df['date_time'].isin(holidays), :]\n",
    "print(f'After removing holidays, {len(df)} visits on {df.osm_id.nunique()} locations from {df.device_aid.nunique()} devices.')\n",
    "\n",
    "# Add pt stations\n",
    "df = pd.merge(df, df_pt, on='osm_id', how='left')\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(subset=['device_aid', 'date', 'dur', 'osm_id'], inplace=True)\n",
    "print(f'After adding public stops, {len(df)} visits on {df.osm_id.nunique()} locations from {df.device_aid.nunique()} devices.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T13:08:22.856999500Z",
     "start_time": "2024-06-07T13:08:21.776699800Z"
    }
   },
   "id": "6307c1c0af704a09"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For precipitation data: 223456 pairs on 60328 locations from 450 days.\n"
     ]
    }
   ],
   "source": [
    "# Add precipitation information\n",
    "df_rain = df.drop_duplicates(subset=['date_time', 'osm_id'])[['date_time', 'osm_id']]\n",
    "print(f'For precipitation data: {len(df_rain)} pairs on {df_rain.osm_id.nunique()} locations from {df_rain.date_time.nunique()} days.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:32:27.174861300Z",
     "start_time": "2024-06-07T12:32:26.980876500Z"
    }
   },
   "id": "53c8d8dfe292c019"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "   date_time       osm_id                                         station_id\n0 2022-06-20  404916112.0  [15523, 2249, 3939, 15156, 14028, 1251, 14304,...\n1 2022-07-11  404916112.0  [15523, 2249, 3939, 15156, 14028, 1251, 14304,...\n2 2022-07-14  695230966.0  [14043, 14079, 14032, 14049, 19100, 161, 13691...\n3 2022-07-16  404057116.0  [2249, 15523, 3939, 15156, 1251, 19244, 14028,...\n4 2022-07-23  404973915.0  [2249, 3939, 15156, 1251, 14304, 19244, 13778,...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_time</th>\n      <th>osm_id</th>\n      <th>station_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-06-20</td>\n      <td>404916112.0</td>\n      <td>[15523, 2249, 3939, 15156, 14028, 1251, 14304,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-07-11</td>\n      <td>404916112.0</td>\n      <td>[15523, 2249, 3939, 15156, 14028, 1251, 14304,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-07-14</td>\n      <td>695230966.0</td>\n      <td>[14043, 14079, 14032, 14049, 19100, 161, 13691...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-07-16</td>\n      <td>404057116.0</td>\n      <td>[2249, 15523, 3939, 15156, 1251, 19244, 14028,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-07-23</td>\n      <td>404973915.0</td>\n      <td>[2249, 3939, 15156, 1251, 14304, 19244, 13778,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rain = pd.merge(df_rain, df_poi2p, on='osm_id', how='left')\n",
    "df_rain.loc[:, 'station_id'] = df_rain.loc[:, 'station_id'].apply(lambda x: [int(j) for j in x.split(',') if j != ''])\n",
    "df_rain.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:32:28.742837300Z",
     "start_time": "2024-06-07T12:32:27.340859800Z"
    }
   },
   "id": "ba644ef9580a0600"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for precipitation: 100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "df_rec_list = []\n",
    "for i in tqdm(range(0, 15), desc='Searching for precipitation'):\n",
    "    df_rain.loc[:, 'station'] = df_rain.loc[:, 'station_id'].apply(lambda x: x[i] if len(x) > i else None)\n",
    "    df_rec = pd.merge(df_rain[['date_time', 'osm_id', 'station']],\n",
    "                      df_p[['date_time', 'station_id', 'precipitation']].rename(columns={'station_id': 'station'}),\n",
    "                      on=['station', 'date_time'], how='left')\n",
    "    df_rec.dropna(inplace=True)\n",
    "    df_rec_list.append(df_rec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:45:41.394039600Z",
     "start_time": "2024-06-07T12:45:37.216112900Z"
    }
   },
   "id": "1ec0a6c8f1a8547b"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For precipitation data: 98.34445016054354 % have records.\n"
     ]
    }
   ],
   "source": [
    "df_rec = pd.concat(df_rec_list)\n",
    "df_rec.drop_duplicates(subset=['date_time', 'osm_id'], keep='first', inplace=True)\n",
    "print(f'For precipitation data: {len(df_rec)/len(df_rain)*100} % have records.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:45:56.196648900Z",
     "start_time": "2024-06-07T12:45:55.879657700Z"
    }
   },
   "id": "92255bd8c7262b03"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "   date_time       osm_id  station  precipitation\n3 2022-07-16  404057116.0   2249.0            0.0\n4 2022-07-23  404973915.0   2249.0            0.1\n5 2022-08-06  404973915.0   2249.0            0.0\n8 2022-08-13  404974714.0   2249.0            0.0\n9 2022-08-16  404973915.0   2249.0            0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_time</th>\n      <th>osm_id</th>\n      <th>station</th>\n      <th>precipitation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>2022-07-16</td>\n      <td>404057116.0</td>\n      <td>2249.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-07-23</td>\n      <td>404973915.0</td>\n      <td>2249.0</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2022-08-06</td>\n      <td>404973915.0</td>\n      <td>2249.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2022-08-13</td>\n      <td>404974714.0</td>\n      <td>2249.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2022-08-16</td>\n      <td>404973915.0</td>\n      <td>2249.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:46:29.467579800Z",
     "start_time": "2024-06-07T12:46:29.334585900Z"
    }
   },
   "id": "b7bab8a06f3d969c"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding precipitation, 311700 visits on 59814 locations from 7638 devices.\n"
     ]
    }
   ],
   "source": [
    "# Add precipitation\n",
    "df = pd.merge(df, df_rec[['osm_id', 'date_time', 'precipitation']], on=['osm_id', 'date_time'], how='left')\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(subset=['device_aid', 'date', 'dur', 'osm_id'], inplace=True)\n",
    "print(f'After adding precipitation, {len(df)} visits on {df.osm_id.nunique()} locations from {df.device_aid.nunique()} devices.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:47:42.029052100Z",
     "start_time": "2024-06-07T12:47:41.585432200Z"
    }
   },
   "id": "27bd77bc420d5425"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Scale up the processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c498fbbb1f2d7506"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_d2f = pd.read_sql(\"\"\"SELECT device_aid, station_id AS station_id_f FROM fuel_station WHERE station_num > 0;\"\"\", con=engine)\n",
    "df_fp = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        station_uuid AS station_f, \n",
    "        date, \n",
    "        CASE \n",
    "            WHEN e5 IS NULL THEN e10\n",
    "            WHEN e10 IS NULL THEN e5\n",
    "            ELSE (e5 + e10) / 2.0 \n",
    "        END AS ef\n",
    "    FROM fuel_station_price;\n",
    "\"\"\", con=engine)\n",
    "df_fp.loc[:, 'date_time'] = pd.to_datetime(df_fp['date'].astype(str), format='%Y-%m-%d')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T18:49:09.648292100Z",
     "start_time": "2024-11-30T18:47:50.362621400Z"
    }
   },
   "id": "d94d55aecab3751e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def visitation_enrichment(data=None):\n",
    "    # Time processing\n",
    "    data.loc[:, 'date_time'] = pd.to_datetime(data['date'].astype(str), format='%Y-%m-%d')\n",
    "    #print(f'{len(data)} visits on {data.osm_id.nunique()} locations from {data.device_aid.nunique()} devices.')\n",
    "\n",
    "    # Filter out national holidays\n",
    "    data.loc[:, 'date_time'] = data.loc[~data['date_time'].isin(holidays), :]\n",
    "    #print(f'After removing holidays, {len(data)} visits on {data.osm_id.nunique()} locations from {data.device_aid.nunique()} devices.')\n",
    "    \n",
    "    # Add pt stations\n",
    "    data = pd.merge(data, df_pt, on='osm_id', how='left')\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop_duplicates(subset=['device_aid', 'date', 'dur', 'osm_id'], inplace=True)\n",
    "    #print(f'After adding public stops, {len(data)} visits on {data.osm_id.nunique()} locations from {data.device_aid.nunique()} devices.')\n",
    "    \n",
    "    # Add precipitation information\n",
    "    df_rain = data.drop_duplicates(subset=['date_time', 'osm_id'])[['date_time', 'osm_id']]\n",
    "    #print(f'For precipitation data: {len(df_rain)} pairs on {df_rain.osm_id.nunique()} locations from {df_rain.date_time.nunique()} days.')\n",
    "    df_rain = pd.merge(df_rain, df_poi2p, on='osm_id', how='left')\n",
    "    df_rain.loc[:, 'station_id'] = df_rain.loc[:, 'station_id'].apply(lambda x: [int(j) for j in x.split(',') if j != ''])\n",
    "    \n",
    "    df_rec_list = []\n",
    "    for i in range(0, 15):\n",
    "        df_rain.loc[:, 'station'] = df_rain.loc[:, 'station_id'].apply(lambda x: x[i] if len(x) > i else None)\n",
    "        df_rec = pd.merge(df_rain[['date_time', 'osm_id', 'station']],\n",
    "                          df_p[['date_time', 'station_id', 'precipitation']].rename(columns={'station_id': 'station'}),\n",
    "                          on=['station', 'date_time'], how='left')\n",
    "        df_rec.dropna(inplace=True)\n",
    "        df_rec_list.append(df_rec)\n",
    "    df_rec = pd.concat(df_rec_list)\n",
    "    df_rec.drop_duplicates(subset=['date_time', 'osm_id'], keep='first', inplace=True)\n",
    "    #print(f'For precipitation data: {len(df_rec)/len(df_rain)*100} % have records.')\n",
    "    # Add precipitation\n",
    "    data = pd.merge(data, df_rec[['osm_id', 'date_time', 'precipitation']], on=['osm_id', 'date_time'], how='left')\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Add fuel price information\n",
    "    df_fuel = data.drop_duplicates(subset=['date_time', 'device_aid'])[['date_time', 'device_aid']]\n",
    "    df_fuel = pd.merge(df_fuel, df_d2f, on='device_aid', how='left')\n",
    "    df_fuel.dropna(inplace=True)\n",
    "    df_fuel.loc[:, 'station_id_f'] = df_fuel.loc[:, 'station_id_f'].apply(lambda x: [j for j in x.split(',') if j != ''])\n",
    "\n",
    "    df_rec_list = []\n",
    "    for i in range(0, 5):\n",
    "        df_fuel.loc[:, 'station_f'] = df_fuel.loc[:, 'station_id_f'].apply(lambda x: x[i] if len(x) > i else None)\n",
    "        df_rec_f = pd.merge(df_fuel[['date_time', 'device_aid', 'station_f']],\n",
    "                          df_fp[['date_time', 'station_f', 'ef']],\n",
    "                          on=['date_time', 'station_f'], how='left')\n",
    "        df_rec_f.dropna(inplace=True)\n",
    "        df_rec_list.append(df_rec_f)\n",
    "    df_rec_f = pd.concat(df_rec_list)\n",
    "    df_rec_f.drop_duplicates(subset=['date_time', 'device_aid'], keep='first', inplace=True)\n",
    "\n",
    "    median_ef = df_rec_f[['device_aid', 'date_time', 'ef']].groupby('date_time')['ef'].median().reset_index()\n",
    "    median_ef_s = df_rec_f['ef'].median()\n",
    "    data = pd.merge(data, df_rec_f[['device_aid', 'date_time', 'ef']], on=['device_aid', 'date_time'], how='left')\n",
    "    # Merge the median values into the main data\n",
    "    data = pd.merge(data, median_ef, on='date_time', how='left', suffixes=('', '_median'))\n",
    "\n",
    "    # Fill NaN values in 'ef' with the median values\n",
    "    data['ef'] = data['ef'].fillna(data['ef_median'])\n",
    "    data['ef'] = data['ef'].fillna(median_ef_s)\n",
    "    # Drop the temporary 'ef_median' column if no longer needed\n",
    "    data = data.drop(columns=['ef_median'])\n",
    "    data.drop_duplicates(subset=['device_aid', 'date', 'dur', 'osm_id'], inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    return data, list(data.osm_id.unique()), data.device_aid.nunique(), len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T07:59:56.921093500Z",
     "start_time": "2024-12-01T07:59:56.782130300Z"
    }
   },
   "id": "a5d6605eb37391ae"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 300/300 [6:32:26<00:00, 78.49s/it]  \n"
     ]
    }
   ],
   "source": [
    "osm_id_list = []\n",
    "devices_count = 0\n",
    "visits_count = 0\n",
    "upper_reso = 3\n",
    "for k, v in tqdm(paths2stops.items(), desc='Processing batches'):\n",
    "    df = pd.read_parquet(v)\n",
    "    df_processed, osms, no_devices, no_visits = visitation_enrichment(data=df)\n",
    "    osm_id_list += osms\n",
    "    devices_count += no_devices\n",
    "    visits_count += no_visits\n",
    "    df_processed.loc[:, f'h3_parent_{upper_reso}'] = df_processed['h3_id'].apply(lambda x: h3.cell_to_parent(x, upper_reso))\n",
    "    df_processed.to_parquet(f'dbs/combined_hex2visits_day_did/stops_{k}.parquet', index=False)\n",
    "osm_id_list = list(set(osm_id_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T14:34:35.135157800Z",
     "start_time": "2024-12-01T08:01:52.154189400Z"
    }
   },
   "id": "b8185b355d3d2c54"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, 420540112 visits to 798068 unique locations from 22138925 devices are stored.\n"
     ]
    }
   ],
   "source": [
    "print(f\"In total, {visits_count} visits to {len(osm_id_list)} unique locations from {devices_count} devices are stored.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T14:34:35.462209100Z",
     "start_time": "2024-12-01T14:34:35.113162400Z"
    }
   },
   "id": "5eb4aec244f5b38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Re-group hexagon results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dfbb4ca03befc4e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'dbs/combined_hex2visits_day_did/stops_0.parquet'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('dbs/combined_hex2visits_day_did/')\n",
    "paths2stops = {int(x.split('_')[-1].split('.')[0]): os.path.join(data_folder, x)\\\n",
    "               for x in list(os.walk(data_folder))[0][2]}\n",
    "paths2stops_list = list(paths2stops.values())\n",
    "paths2stops_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T18:52:24.293132300Z",
     "start_time": "2024-12-01T18:52:24.171409800Z"
    }
   },
   "id": "ac266df999ce5527"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n",
      "100%|██████████| 49/49 [02:31<00:00,  3.10s/it]\n",
      "100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n",
      "100%|██████████| 49/49 [02:33<00:00,  3.14s/it]\n",
      "100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n",
      "100%|██████████| 49/49 [02:33<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "for n in (0, 1, 2):\n",
    "    df = pd.concat([pd.read_parquet(file) for file in tqdm(paths2stops_list[n*100 : (n+1)*100])], ignore_index=True)\n",
    "    def save_g(group):\n",
    "        group.to_parquet(os.path.join(f'dbs/combined_hex2visits_day_did_g/h_{group.name}_{n}.parquet'), index=False)\n",
    "    tqdm.pandas()\n",
    "    df.groupby('h3_parent_3').progress_apply(save_g, include_groups=False)\n",
    "    del df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T19:05:17.513965100Z",
     "start_time": "2024-12-01T18:52:37.084362400Z"
    }
   },
   "id": "c47673c1a16d2bac"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'831f33fffffffff'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('dbs/combined_hex2visits_day_did_g/')\n",
    "h3_parent_list = list(set([x.split('_')[1] for x in list(os.walk(data_folder))[0][2]]))\n",
    "h3_parent_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T19:05:17.617679100Z",
     "start_time": "2024-12-01T19:05:17.526400900Z"
    }
   },
   "id": "19f9dfc1180e2e14"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging h3 parent grid: 100%|██████████| 49/49 [09:03<00:00, 11.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for h3_p in tqdm(h3_parent_list, desc='Merging h3 parent grid'):\n",
    "    h3_parent_files = [f\"dbs/combined_hex2visits_day_did_g/h_{x}_{y}.parquet\" \n",
    "                       for x, y in zip([h3_p]*3, [0, 1, 2])]\n",
    "    df = pd.concat([pd.read_parquet(file) for file in h3_parent_files], ignore_index=True)\n",
    "    df.to_parquet(os.path.join(f'dbs/combined_hex2visits_day_did_g/h_{h3_p}.parquet'), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T19:14:21.350548600Z",
     "start_time": "2024-12-01T19:05:17.617679100Z"
    }
   },
   "id": "8ea5515ef68c7e38"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['dbs/combined_hex2visits_day_did_g/h_831f33fffffffff_0.parquet',\n 'dbs/combined_hex2visits_day_did_g/h_831f33fffffffff_1.parquet',\n 'dbs/combined_hex2visits_day_did_g/h_831f33fffffffff_2.parquet',\n 'dbs/combined_hex2visits_day_did_g/h_831f12fffffffff_0.parquet']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3_parent_files = []\n",
    "for h3_p in h3_parent_list:\n",
    "    h3_parent_files += [f\"dbs/combined_hex2visits_day_did_g/h_{x}_{y}.parquet\" \n",
    "                        for x, y in zip([h3_p]*3, [0, 1, 2])]\n",
    "h3_parent_files[:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T19:14:21.445058100Z",
     "start_time": "2024-12-01T19:14:21.334589900Z"
    }
   },
   "id": "e562c8676e56f0b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for ele in h3_parent_files:\n",
    "    os.remove(ele)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T19:14:23.281365300Z",
     "start_time": "2024-12-01T19:14:21.429414400Z"
    }
   },
   "id": "62ed5a3d57da7296"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Individual information enrichment (Hex)\n",
    "Existing: grdi, wt_p\n",
    "To add: pop_100m, age_100m, net_rent_100m, foreign_share_100m*\n",
    "*Needs to extract from external file, not in the database."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "654509b7608ae4e0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'dbs/combined_hex2visits_day_did_g/h_831e26fffffffff.parquet'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('dbs/combined_hex2visits_day_did_g/')\n",
    "h3_parent_list = [data_folder + x for x in list(os.walk(data_folder))[0][2]]\n",
    "h3_parent_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T19:37:04.832753600Z",
     "start_time": "2024-11-26T19:37:04.713743700Z"
    }
   },
   "id": "b3d744f1ed9dd95"
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(h3_parent_list[0])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d888b74fcbc336",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 5.1 Load individual data",
   "metadata": {
    "collapsed": false
   },
   "id": "204fecf05a3a5c9f"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "df_h = pd.read_sql(\"\"\"SELECT device_aid, pop_100m, pop_1km, age_100m, net_rent_100m, grdi FROM home_g;\"\"\", con=engine)\n",
    "df_hc = pd.read_sql(\"\"\"SELECT device_aid, latitude, longitude FROM home;\"\"\", con=engine)\n",
    "df_h = pd.merge(df_h, df_hc, on='device_aid', how='left')\n",
    "del df_hc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:18:19.270448700Z",
     "start_time": "2024-11-06T14:14:50.564690100Z"
    }
   },
   "id": "5307c374fa7945ce"
  },
  {
   "cell_type": "code",
   "source": [
    "gdf_h = workers.df2gdf_point(df=df_h, x_field='longitude', y_field='latitude', crs=4326, drop=True).to_crs(3035)\n",
    "gdf_h.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf780fcaa201b24",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add foreign info."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "744ce320262e0f4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('dbs/geo/census_2022/Auslaenderanteil_in_Gitterzellen/Zensus2022_Anteil_Auslaender_100m-Gitter.csv',\n",
    "                   sep=';', encoding='utf-8')\n",
    "df_c.columns = ['grid', 'x_mp_100m', 'y_mp_100m', 'f_share', 'un']\n",
    "df_c = df_c[['grid', 'x_mp_100m', 'y_mp_100m', 'f_share']]\n",
    "df_c['f_share'] = df_c['f_share'].replace('–', 0)  # Replace '–' with NaN\n",
    "df_c.loc[:, 'f_share'] = df_c.loc[:, 'f_share'].str.replace(',', '.').astype(float)\n",
    "df_c.fillna(0, inplace=True)\n",
    "# Apply function to create geometry column\n",
    "df_c['geometry'] = df_c.apply(lambda row: workers.create_square(row['x_mp_100m'], row['y_mp_100m']), axis=1)\n",
    "# Convert to GeoDataFrame\n",
    "gdf_c = gpd.GeoDataFrame(df_c, geometry='geometry', crs=\"EPSG:3035\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-06T14:14:45.192034400Z"
    }
   },
   "id": "c4245f7a3e5ee9e7"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "22741786"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_h = gdf_h.sjoin(gdf_c[['f_share', 'geometry']], how='left')\n",
    "len(gdf_h)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:24:28.416353200Z",
     "start_time": "2024-11-06T14:22:30.463714300Z"
    }
   },
   "id": "4c332d4b43e3bbf5"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "gdf_h.drop(columns=['index_right'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:24:30.849315800Z",
     "start_time": "2024-11-06T14:24:28.408014900Z"
    }
   },
   "id": "fcb6eca98c1f5b70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_w = pd.read_sql(\"\"\"SELECT * FROM weight;\"\"\", con=engine)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df2ad22af962fe41"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "gdf_h = pd.merge(gdf_h, df_w, on='device_aid', how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:25:20.209112500Z",
     "start_time": "2024-11-06T14:24:30.849315800Z"
    }
   },
   "id": "7e81916f6f271aed"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "var_stats_list = []\n",
    "for var in ['pop_100m', 'pop_1km', 'age_100m', 'net_rent_100m', 'f_share', 'grdi']:\n",
    "    ## Weighted percentiles\n",
    "    na_l = len(gdf_h.loc[gdf_h[var].isna(), [var]])\n",
    "    data = gdf_h.loc[~gdf_h[var].isna(), [var, 'wt_p']].copy()\n",
    "    d, wt = data[var], data['wt_p']\n",
    "    wdf = DescrStatsW(d, weights=wt, ddof=1)\n",
    "    sts = wdf.quantile([0.25, 0.5, 0.75])\n",
    "    bds = sts.values\n",
    "    var_stats_list.append((var, na_l, bds[0], bds[1], bds[2]))\n",
    "df_var_stats = pd.DataFrame(var_stats_list, columns=['var', 'missing', 'q25', 'q50', 'q75'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:25:29.432048900Z",
     "start_time": "2024-11-06T14:25:20.224381900Z"
    }
   },
   "id": "20e55bd76c59e815"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "             var  missing         q25          q50         q75\n0       pop_100m  5618795   24.000000    46.000000    93.00000\n1        pop_1km   303347  742.000000  2007.000000  4148.00000\n2       age_100m  5618796   38.000000    42.000000    48.00000\n3  net_rent_100m  9106688    5.000000     6.000000     8.00000\n4        f_share  5618795    0.000000     8.700000    22.06000\n5           grdi        0    3.374868     9.876637    29.29395",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var</th>\n      <th>missing</th>\n      <th>q25</th>\n      <th>q50</th>\n      <th>q75</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pop_100m</td>\n      <td>5618795</td>\n      <td>24.000000</td>\n      <td>46.000000</td>\n      <td>93.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pop_1km</td>\n      <td>303347</td>\n      <td>742.000000</td>\n      <td>2007.000000</td>\n      <td>4148.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>age_100m</td>\n      <td>5618796</td>\n      <td>38.000000</td>\n      <td>42.000000</td>\n      <td>48.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>net_rent_100m</td>\n      <td>9106688</td>\n      <td>5.000000</td>\n      <td>6.000000</td>\n      <td>8.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f_share</td>\n      <td>5618795</td>\n      <td>0.000000</td>\n      <td>8.700000</td>\n      <td>22.06000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>grdi</td>\n      <td>0</td>\n      <td>3.374868</td>\n      <td>9.876637</td>\n      <td>29.29395</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_var_stats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:25:29.572679800Z",
     "start_time": "2024-11-06T14:25:29.432048900Z"
    }
   },
   "id": "bd6a76abf444b03f"
  },
  {
   "cell_type": "code",
   "source": [
    "for var, cat in tqdm(zip(['pop_1km', 'age_100m', 'net_rent_100m', 'f_share', 'grdi'],\n",
    "                    ['pop_density', 'age', 'net_rent', 'birth_f', 'deprivation'])):\n",
    "    grp_b = df_var_stats.loc[df_var_stats['var'] == var, ['q25', 'q50', 'q75']].values[0].tolist()\n",
    "    if var != 'f_share':\n",
    "        grp_b = sorted([gdf_h[var].min()-1] + grp_b + [gdf_h[var].max()+1])  # Full range\n",
    "        labels=[\"q1\", \"q2\", \"q3\", 'q4']\n",
    "    else:\n",
    "        grp_b = sorted([-1, 8.7, 22.06, 101])  # Full range\n",
    "        labels=[\"q12\", \"q3\", 'q4']\n",
    "    p1 = gdf_h.loc[~gdf_h[var].isna(), :]\n",
    "    p2 = gdf_h.loc[gdf_h[var].isna(), :]\n",
    "    p1[cat] = pd.cut(p1[var], grp_b, labels=labels)\n",
    "    p2[cat] = 'na'\n",
    "    gdf_h = pd.concat([p1, p2])\n",
    "gdf_h.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75b82fbf8d5102f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "22960172"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_h[['device_aid', 'f_share', 'wt_p', 'pop_density', \n",
    "       'age', 'net_rent', 'birth_f', 'deprivation']].to_sql(f'device_grp', engine, index=False, \n",
    "                                                            if_exists='replace', method='multi', chunksize=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:35:19.705614200Z",
     "start_time": "2024-11-06T14:35:10.028625800Z"
    }
   },
   "id": "35b5fd4cffe2a1ec"
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Daily visitation calculation - hex x device group",
   "metadata": {
    "collapsed": false
   },
   "id": "533058e1e7e9335c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_ind = pd.read_sql(\"\"\"SELECT * FROM device_grp;\"\"\", con=engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T19:01:23.571543600Z",
     "start_time": "2024-11-06T18:59:30.205490100Z"
    }
   },
   "id": "62ddefbc58d2550d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'dbs/combined_hex2visits_day_did_g/h_831e26fffffffff.parquet'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('dbs/combined_hex2visits_day_did_g/')\n",
    "hex_file_list = [data_folder + x for x in list(os.walk(data_folder))[0][2]]\n",
    "hex_file_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T19:01:23.649311500Z",
     "start_time": "2024-11-06T19:01:23.539940500Z"
    }
   },
   "id": "e773227478312ac7"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def visit_patterns_hex(data):\n",
    "    data.loc[:, 'date'] = data.loc[:, 'date'].astype(str)\n",
    "    metrics_dict = dict()\n",
    "    # osm_id info\n",
    "    for var in ('year', 'month', 'weekday'):\n",
    "        metrics_dict[var] = data[var].values[0]\n",
    "    # Spatial characteristics\n",
    "    for var in ('precipitation', 'pt_station_num'):\n",
    "        metrics_dict[var] = np.average(data[var], weights=data['wt_p'])\n",
    "    # Visits\n",
    "    metrics_dict['num_visits_wt'] = data['wt_p'].sum()\n",
    "    metrics_dict['num_unique_device'] = data.device_aid.nunique()\n",
    "\n",
    "    # Individuals in the origins\n",
    "    metrics_dict['f_share'] = np.average(data.loc[~data['f_share'].isna(), 'f_share'], \n",
    "                                         weights=data.loc[~data['f_share'].isna(), 'wt_p'])\n",
    "    metrics_dict['grdi'] = np.average(data.loc[~data['grdi'].isna(), 'grdi'], \n",
    "                                         weights=data.loc[~data['grdi'].isna(), 'wt_p'])\n",
    "    \n",
    "    ## weighted average\n",
    "    d, wt = data.loc[data['d_h'] > 0, 'd_h'], data.loc[data['d_h'] > 0, 'wt_p']\n",
    "    d_lg = d.apply(lambda x: np.log10(x))\n",
    "    metrics_dict['d_ha_wt'] = 10 ** np.average(d_lg, weights=wt)\n",
    "    return pd.Series(metrics_dict)  # pd.DataFrame(metrics_dict, index=[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T19:45:05.842470200Z",
     "start_time": "2024-11-06T19:45:05.711823Z"
    }
   },
   "id": "f5c82c512deee950"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15105/15105 [00:39<00:00, 378.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for f, i in zip(hex_file_list, range(1, len(hex_file_list) + 1)):\n",
    "    print(f'File {i}/{len(hex_file_list)}...')\n",
    "    df = pd.read_parquet(hex_file_list[0])\n",
    "    df = pd.merge(df, df_ind.drop(columns=['wt_p']), on='device_aid', how='left')\n",
    "    tqdm.pandas()\n",
    "    df_v = df.groupby(['h3_id', 'date']).progress_apply(visit_patterns_hex).reset_index()\n",
    "    df_v.loc[:, 'group'] = 'all'\n",
    "    df_v.loc[:, 'level'] = 'all'\n",
    "    df_v_list = [df_v]\n",
    "    for v in ( 'pop_density', 'age', 'net_rent', 'birth_f', 'deprivation'):\n",
    "        tqdm.pandas()\n",
    "        df_v = df.groupby(['h3_id', 'date', v]).progress_apply(visit_patterns_hex).reset_index()\n",
    "        df_v.rename(columns={v: 'level'}, inplace=True)\n",
    "        df_v.loc[:, 'group'] = v\n",
    "        df_v_list.append(df_v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T19:35:12.059532600Z",
     "start_time": "2024-11-06T19:34:31.985272700Z"
    }
   },
   "id": "1191443c7358b024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time-shifted DiD place filtering (hexagons-h3-7)\n",
    "Data: daily visitation statistics stored under `dbs/combined_visits_day_did_hex/` categorized by area."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "801048b440937c8f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\nine-euro-ticket-de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd D:\\nine-euro-ticket-de"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T15:31:25.572173Z",
     "start_time": "2024-11-13T15:31:25.507629100Z"
    }
   },
   "id": "17609e14ee0d8ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import h3\n",
    "import workers\n",
    "import tdid\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T15:31:31.256259700Z",
     "start_time": "2024-11-13T15:31:25.572173Z"
    }
   },
   "id": "1cb8ecd62626ca5c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Data location\n",
    "user = workers.keys_manager['database']['user']\n",
    "password = workers.keys_manager['database']['password']\n",
    "port = workers.keys_manager['database']['port']\n",
    "db_name = workers.keys_manager['database']['name']\n",
    "engine = sqlalchemy.create_engine(f'postgresql://{user}:{password}@localhost:{port}/{db_name}?gssencmode=disable')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T15:31:31.576592700Z",
     "start_time": "2024-11-13T15:31:31.264089400Z"
    }
   },
   "id": "e4f985301b3cf59b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbs/combined_visits_day_did_hex/h_831e26fffffffff.parquet\n"
     ]
    }
   ],
   "source": [
    "# Hexagon data\n",
    "data_folder = 'dbs/combined_visits_day_did_hex/'\n",
    "paths2hex = {x.split('.')[0]: os.path.join(data_folder, x)\n",
    "             for x in list(os.walk(data_folder))[0][2]}\n",
    "paths2hex_list = [v for k, v in paths2hex.items()]\n",
    "print(paths2hex_list[0])\n",
    "\n",
    "# Target folder to save the data for DiD modeling\n",
    "target_folder = 'dbs/combined_did_data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T15:31:31.701956100Z",
     "start_time": "2024-11-13T15:31:31.576592700Z"
    }
   },
   "id": "b4beafd52bf171f3"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "grp, lv = 'age', 'q4'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:04:48.704064400Z",
     "start_time": "2024-11-13T21:04:48.563330900Z"
    }
   },
   "id": "2e69d1f082b09635"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load hexagon visit patterns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a97bd6981265a797"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load hexagons: 100%|██████████| 49/49 [02:11<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By group age Level q4\n",
      "No. of unique hexagons included for analysis - 9ET: 30430\n",
      "No. of unique hexagons included for analysis - DT: 34169\n"
     ]
    }
   ],
   "source": [
    "cols = ['h3_id', 'date', 'year', 'month', 'weekday', 'precipitation',\n",
    "       'pt_station_num', 'num_visits_wt', 'num_unique_device', 'd_ha_wt', \n",
    "        'group', 'level']\n",
    "# Load hexagons\n",
    "df_list = []\n",
    "for lb in tqdm(paths2hex_list, desc='Load hexagons'):\n",
    "    df = pd.read_parquet(lb, columns=cols)\n",
    "    df = df.loc[(df.num_unique_device > 3) & (df.num_unique_device > 3) & (df['month'] != 9) &\n",
    "                (df.group == grp) & (df.level == lv), :]\n",
    "    df_list.append(df.drop(columns=['group', 'level']))\n",
    "df = pd.concat(df_list)\n",
    "del df_list\n",
    "\n",
    "print('By group', grp, 'Level', lv)\n",
    "# The 9ET\n",
    "df1 = df.loc[(df['year'].isin([2019, 2022])) & (df['month'].isin([5, 6, 7, 8])), :].copy()\n",
    "print(f\"No. of unique hexagons included for analysis - 9ET: {df1['h3_id'].nunique()}\")\n",
    "df1['date'] = df1['date'].astype(str)\n",
    "\n",
    "# The D-ticket\n",
    "df2 = df.loc[(df['year'].isin([2022, 2023])) & (df['month'].isin([2, 3, 4, 5])), :].copy()\n",
    "print(f\"No. of unique hexagons included for analysis - DT: {df2['h3_id'].nunique()}\")\n",
    "df2['date'] = df2['date'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:05.213947700Z",
     "start_time": "2024-11-13T21:04:49.548548500Z"
    }
   },
   "id": "1d3251f329dd6258"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37384\n"
     ]
    }
   ],
   "source": [
    "h3_id_list = list(set(list(df1['h3_id'].unique()) + list(df2['h3_id'].unique())))\n",
    "print(len(h3_id_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:05.526447600Z",
     "start_time": "2024-11-13T21:07:05.213947700Z"
    }
   },
   "id": "a26f525141760175"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Connect hexagon (centroids) with state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5788fd3be5c98f"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37384/37384 [00:00<00:00, 341807.00it/s]\n"
     ]
    }
   ],
   "source": [
    "df_h3 = pd.DataFrame(h3_id_list, columns=['h3_id'])\n",
    "# Step 1: Convert each H3 index to its centroid coordinates (lat, lon)\n",
    "tqdm.pandas()\n",
    "df_h3['centroid'] = df_h3['h3_id'].progress_apply(lambda x: h3.h3_to_geo(x))\n",
    "\n",
    "# Step 2: Split the centroid coordinates into separate latitude and longitude columns\n",
    "df_h3[['lat', 'lon']] = pd.DataFrame(df_h3['centroid'].tolist(), index=df_h3.index)\n",
    "\n",
    "# Step 3: Create a GeoDataFrame using these coordinates as Point geometries\n",
    "geometry = [Point(xy) for xy in zip(df_h3['lon'], df_h3['lat'])]\n",
    "gdf = gpd.GeoDataFrame(df_h3, geometry=geometry)\n",
    "\n",
    "# Optional: Set the CRS to WGS84 (EPSG:4326)\n",
    "gdf = gdf.set_crs(epsg=4326)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:06.167092200Z",
     "start_time": "2024-11-13T21:07:05.526447600Z"
    }
   },
   "id": "8aee14de0012fceb"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Find h3_id: state\n",
    "gdf_state = gpd.read_file(\"dbs/geo/vg2500_12-31.utm32s.shape/vg2500/vg2500_LAN.shp\").to_crs(4326)\n",
    "gdf_state = gdf_state.loc[gdf_state['GF'] == 9, :].rename(columns={'GEN': 'state'})\n",
    "states = gdf.sjoin(gdf_state[['state', 'geometry']])\n",
    "states.dropna(inplace=True)\n",
    "states = states[['h3_id', 'state']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:07.169411Z",
     "start_time": "2024-11-13T21:07:06.136198Z"
    }
   },
   "id": "cb1367104f3f36bf"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of locations: 37384\n"
     ]
    },
    {
     "data": {
      "text/plain": "             h3_id                                 centroid        lat  \\\n0  881fab8117fffff  (49.58520779939761, 10.619123420680415)  49.585208   \n1  881f1386adfffff  (52.164242614196034, 9.784649037487563)  52.164243   \n2  881fa5add1fffff   (51.57032761291947, 6.962343593247691)  51.570328   \n3  881fac3b37fffff   (50.74194802044497, 9.259039158849113)  50.741948   \n4  881faa70e9fffff  (48.863326460510926, 9.258221332697397)  48.863326   \n\n         lon                   geometry                state  \n0  10.619123  POINT (10.61912 49.58521)               Bayern  \n1   9.784649   POINT (9.78465 52.16424)        Niedersachsen  \n2   6.962344   POINT (6.96234 51.57033)  Nordrhein-Westfalen  \n3   9.259039   POINT (9.25904 50.74195)               Hessen  \n4   9.258221   POINT (9.25822 48.86333)    Baden-Württemberg  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h3_id</th>\n      <th>centroid</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>geometry</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>881fab8117fffff</td>\n      <td>(49.58520779939761, 10.619123420680415)</td>\n      <td>49.585208</td>\n      <td>10.619123</td>\n      <td>POINT (10.61912 49.58521)</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>881f1386adfffff</td>\n      <td>(52.164242614196034, 9.784649037487563)</td>\n      <td>52.164243</td>\n      <td>9.784649</td>\n      <td>POINT (9.78465 52.16424)</td>\n      <td>Niedersachsen</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>881fa5add1fffff</td>\n      <td>(51.57032761291947, 6.962343593247691)</td>\n      <td>51.570328</td>\n      <td>6.962344</td>\n      <td>POINT (6.96234 51.57033)</td>\n      <td>Nordrhein-Westfalen</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>881fac3b37fffff</td>\n      <td>(50.74194802044497, 9.259039158849113)</td>\n      <td>50.741948</td>\n      <td>9.259039</td>\n      <td>POINT (9.25904 50.74195)</td>\n      <td>Hessen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>881faa70e9fffff</td>\n      <td>(48.863326460510926, 9.258221332697397)</td>\n      <td>48.863326</td>\n      <td>9.258221</td>\n      <td>POINT (9.25822 48.86333)</td>\n      <td>Baden-Württemberg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = pd.merge(gdf, states, on='h3_id', how='left')\n",
    "print(f'No. of locations: {gdf.h3_id.nunique()}')\n",
    "gdf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:07.337873400Z",
     "start_time": "2024-11-13T21:07:07.169411Z"
    }
   },
   "id": "89191db0ff908bad"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30343 34056\n"
     ]
    }
   ],
   "source": [
    "# Add state\n",
    "df1 = pd.merge(df1, gdf[['h3_id', 'state']], on='h3_id', how='left')\n",
    "df1.dropna(inplace=True)\n",
    "df2 = pd.merge(df2, gdf[['h3_id', 'state']], on='h3_id', how='left')\n",
    "df2.dropna(inplace=True)\n",
    "print(df1.h3_id.nunique(), df2.h3_id.nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:08.582628800Z",
     "start_time": "2024-11-13T21:07:07.337873400Z"
    }
   },
   "id": "1a7347bbfbb8446e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Add fuel price (time-based)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b66801c94fdb3b6"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "df_f = pd.read_sql(\"\"\"SELECT * FROM fuel_price;\"\"\", con=engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:08.739229Z",
     "start_time": "2024-11-13T21:07:08.582628800Z"
    }
   },
   "id": "d58670c62d6fd8f6"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30343 34056\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.merge(df1, df_f[['date', 'gasoline']], on='date', how='left')\n",
    "df2 = pd.merge(df2, df_f[['date', 'gasoline']], on='date', how='left')\n",
    "print(df1.h3_id.nunique(), df2.h3_id.nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:09.634337800Z",
     "start_time": "2024-11-13T21:07:08.739229Z"
    }
   },
   "id": "f24758b1ee11563b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Select complete data and save"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73dbbe23f6ae7b1a"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30343/30343 [00:10<00:00, 2945.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of h3 grids complete for the 9ET 10392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34056/34056 [00:10<00:00, 3200.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of h3 grids complete for the D-Ticket 22742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def h3_stats_ym(data):\n",
    "    # comp = 2 means being complete\n",
    "    comp_y = data['year'].nunique()\n",
    "    return pd.Series(dict(comp_y=comp_y))\n",
    "\n",
    "tqdm.pandas()\n",
    "df1_r= df1.groupby('h3_id').progress_apply(h3_stats_ym).reset_index()\n",
    "print(\"No. of h3 grids complete for the 9ET\", len(df1_r.loc[df1_r.comp_y==2, :]))\n",
    "\n",
    "tqdm.pandas()\n",
    "df2_r = df2.groupby('h3_id').progress_apply(h3_stats_ym).reset_index()\n",
    "print(\"No. of h3 grids complete for the D-Ticket\", len(df2_r.loc[df2_r.comp_y==2, :]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:30.985480500Z",
     "start_time": "2024-11-13T21:07:09.634337800Z"
    }
   },
   "id": "ba0303ec8caaea70"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of h3 grids included for analysis - 9ET: 10392\n",
      "No. of h3 grids included for analysis - DT: 22742\n"
     ]
    }
   ],
   "source": [
    "df1_rh = df1.loc[df1.h3_id.isin(df1_r.loc[df1_r.comp_y==2, 'h3_id'].values), :]\n",
    "df2_rh = df2.loc[df2.h3_id.isin(df2_r.loc[df2_r.comp_y==2, 'h3_id'].values), :]\n",
    "print(f\"No. of h3 grids included for analysis - 9ET: {df1_rh['h3_id'].nunique()}\")\n",
    "print(f\"No. of h3 grids included for analysis - DT: {df2_rh['h3_id'].nunique()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:31.506538Z",
     "start_time": "2024-11-13T21:07:30.985480500Z"
    }
   },
   "id": "f347ee02cbc43165"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "df1_rh.to_parquet(target_folder + f'h3_grids_9et_{grp}_{lv}.parquet', index=False)\n",
    "df2_rh.to_parquet(target_folder + f'h3_grids_dt_{grp}_{lv}.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:33.212647400Z",
     "start_time": "2024-11-13T21:07:31.506538Z"
    }
   },
   "id": "b5bb9dd302104cc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Time series"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5057f95a4134ad8"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "df1_rh = pd.read_parquet(target_folder + f'h3_grids_9et_{grp}_{lv}.parquet')\n",
    "df2_rh = pd.read_parquet(target_folder + f'h3_grids_dt_{grp}_{lv}.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:33.915113100Z",
     "start_time": "2024-11-13T21:07:33.212647400Z"
    }
   },
   "id": "b1cc8d41c1698c2f"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def visit_patterns_hex_date(data):\n",
    "    data.loc[:, 'date'] = data.loc[:, 'date'].astype(str)\n",
    "    metrics_dict = dict()\n",
    "    # osm_id info\n",
    "    for var in ('date', 'year', 'month', 'weekday', 'pt_station_num'):\n",
    "        metrics_dict[var] = data[var].values[0]\n",
    "\n",
    "    # Visits\n",
    "    metrics_dict['visit_50'] = 10 ** (np.log10(data['num_visits_wt']).median())\n",
    "    metrics_dict['visit_25'] = 10 ** (np.nanquantile(np.log10(data['num_visits_wt']), 0.25))\n",
    "    metrics_dict['visit_75'] = 10 ** (np.nanquantile(np.log10(data['num_visits_wt']), 0.75))\n",
    "\n",
    "    # Distance\n",
    "    metrics_dict['d_50'] = 10 ** (np.log10(data['d_ha_wt']).median())\n",
    "    metrics_dict['d_25'] = 10 ** (np.nanquantile(np.log10(data['d_ha_wt']), 0.25))\n",
    "    metrics_dict['d_75'] = 10 ** (np.nanquantile(np.log10(data['d_ha_wt']), 0.75))\n",
    "    return pd.Series(metrics_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:34.086867300Z",
     "start_time": "2024-11-13T21:07:33.915113100Z"
    }
   },
   "id": "52d059d2be63347f"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 322.44it/s]\n",
      "100%|██████████| 231/231 [00:01<00:00, 162.70it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df_v = pd.concat([df1_rh.groupby('date').progress_apply(visit_patterns_hex_date).reset_index(drop=True).assign(policy='9et'),\n",
    "                  df2_rh.groupby('date').progress_apply(visit_patterns_hex_date).reset_index(drop=True).assign(policy='dt')])\n",
    "df_v.to_parquet(os.path.join(f\"results/hex_time_series/{grp}_{lv}.parquet\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:07:36.532253200Z",
     "start_time": "2024-11-13T21:07:34.086867300Z"
    }
   },
   "id": "aab7782ade1d5af9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Entropy balancing\n",
    "### 6.1 The 9ET data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d01e5aaf166d47b"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.5.3                                    \n",
      "===============================================================================\n",
      "(CVXPY) Nov 13 09:32:53 AM: Your problem has 719182 variables, 719185 constraints, and 0 parameters.\n",
      "(CVXPY) Nov 13 09:32:53 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Nov 13 09:32:53 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Nov 13 09:32:53 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "(CVXPY) Nov 13 09:32:53 AM: Your problem is compiled with the CPP canonicalization backend.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:32:53 AM: Compiling problem (target solver=SCS).\n",
      "(CVXPY) Nov 13 09:32:53 AM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> SCS\n",
      "(CVXPY) Nov 13 09:32:53 AM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Nov 13 09:32:53 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Nov 13 09:32:53 AM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Nov 13 09:32:54 AM: Applying reduction SCS\n",
      "(CVXPY) Nov 13 09:32:56 AM: Finished problem compilation (took 2.976e+00 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:32:56 AM: Invoking solver SCS  to obtain a solution.\n",
      "------------------------------------------------------------------\n",
      "\t       SCS v3.2.7 - Splitting Conic Solver\n",
      "\t(c) Brendan O'Donoghue, Stanford University, 2012\n",
      "------------------------------------------------------------------\n",
      "problem:  variables n: 1438364, constraints m: 2876731\n",
      "cones: \t  z: primal zero / dual free vars: 3\n",
      "\t  l: linear vars: 719182\n",
      "\t  e: exp vars: 2157546, dual exp vars: 0\n",
      "settings: eps_abs: 1.0e-05, eps_rel: 1.0e-05, eps_infeas: 1.0e-07\n",
      "\t  alpha: 1.50, scale: 1.00e-01, adaptive_scale: 1\n",
      "\t  max_iters: 5000, normalize: 1, rho_x: 1.00e-06\n",
      "\t  acceleration_lookback: 10, acceleration_interval: 10\n",
      "lin-sys:  sparse-direct-amd-qdldl\n",
      "\t  nnz(A): 4311180, nnz(P): 0\n",
      "------------------------------------------------------------------\n",
      " iter | pri res | dua res |   gap   |   obj   |  scale  | time (s)\n",
      "------------------------------------------------------------------\n",
      "     0| 2.93e+01  8.06e-01  6.72e+06 -3.36e+06  1.00e-01  3.61e+00 \n",
      "    50| 1.85e-05  8.64e-06  4.15e-07 -2.00e+00  1.00e-01  1.42e+01 \n",
      "------------------------------------------------------------------\n",
      "status:  solved\n",
      "timings: total: 1.42e+01s = setup: 2.55e+00s + solve: 1.16e+01s\n",
      "\t lin-sys: 5.06e+00s, cones: 3.93e+00s, accel: 3.73e-01s\n",
      "------------------------------------------------------------------\n",
      "objective = -1.997962\n",
      "------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:33:10 AM: Problem status: optimal\n",
      "(CVXPY) Nov 13 09:33:10 AM: Optimal value: inf\n",
      "(CVXPY) Nov 13 09:33:10 AM: Compilation took 2.976e+00 seconds\n",
      "(CVXPY) Nov 13 09:33:10 AM: Solver (including time spent in interface) took 1.443e+01 seconds\n",
      "Number of unique places: 39133\n"
     ]
    }
   ],
   "source": [
    "var = 'num_visits_wt'\n",
    "df1_rh[f'ln_{var}'] = np.log(df1_rh[var])\n",
    "df1_w = tdid.data_filtering_and_weighting(data=df1_rh, control_y=2019, treatment_y=2022, covar='pt_station_num',\n",
    "                                          control_m=[5,], treatment_m=[6, 7, 8], var=f'ln_{var}', unit='h3')\n",
    "df1_w.to_parquet(target_folder + f'h3_grids_9et_{grp}_{lv}_wt_v.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T08:33:42.897306500Z",
     "start_time": "2024-11-13T08:32:50.038094300Z"
    }
   },
   "id": "75633f95642e8493"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.5.3                                    \n",
      "===============================================================================\n",
      "(CVXPY) Nov 13 09:34:01 AM: Your problem has 719182 variables, 719185 constraints, and 0 parameters.\n",
      "(CVXPY) Nov 13 09:34:01 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Nov 13 09:34:01 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Nov 13 09:34:01 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "(CVXPY) Nov 13 09:34:01 AM: Your problem is compiled with the CPP canonicalization backend.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:34:01 AM: Compiling problem (target solver=SCS).\n",
      "(CVXPY) Nov 13 09:34:01 AM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> SCS\n",
      "(CVXPY) Nov 13 09:34:01 AM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Nov 13 09:34:01 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Nov 13 09:34:01 AM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Nov 13 09:34:03 AM: Applying reduction SCS\n",
      "(CVXPY) Nov 13 09:34:04 AM: Finished problem compilation (took 2.830e+00 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:34:04 AM: Invoking solver SCS  to obtain a solution.\n",
      "------------------------------------------------------------------\n",
      "\t       SCS v3.2.7 - Splitting Conic Solver\n",
      "\t(c) Brendan O'Donoghue, Stanford University, 2012\n",
      "------------------------------------------------------------------\n",
      "problem:  variables n: 1438364, constraints m: 2876731\n",
      "cones: \t  z: primal zero / dual free vars: 3\n",
      "\t  l: linear vars: 719182\n",
      "\t  e: exp vars: 2157546, dual exp vars: 0\n",
      "settings: eps_abs: 1.0e-05, eps_rel: 1.0e-05, eps_infeas: 1.0e-07\n",
      "\t  alpha: 1.50, scale: 1.00e-01, adaptive_scale: 1\n",
      "\t  max_iters: 5000, normalize: 1, rho_x: 1.00e-06\n",
      "\t  acceleration_lookback: 10, acceleration_interval: 10\n",
      "lin-sys:  sparse-direct-amd-qdldl\n",
      "\t  nnz(A): 4311180, nnz(P): 0\n",
      "------------------------------------------------------------------\n",
      " iter | pri res | dua res |   gap   |   obj   |  scale  | time (s)\n",
      "------------------------------------------------------------------\n",
      "     0| 2.93e+01  8.06e-01  6.73e+06 -3.37e+06  1.00e-01  3.64e+00 \n",
      "    50| 7.48e-06  3.13e-06  1.83e-05 -2.00e+00  1.00e-01  1.44e+01 \n",
      "------------------------------------------------------------------\n",
      "status:  solved\n",
      "timings: total: 1.44e+01s = setup: 2.57e+00s + solve: 1.19e+01s\n",
      "\t lin-sys: 5.18e+00s, cones: 4.06e+00s, accel: 3.32e-01s\n",
      "------------------------------------------------------------------\n",
      "objective = -1.997979\n",
      "------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:34:19 AM: Problem status: optimal\n",
      "(CVXPY) Nov 13 09:34:19 AM: Optimal value: inf\n",
      "(CVXPY) Nov 13 09:34:19 AM: Compilation took 2.830e+00 seconds\n",
      "(CVXPY) Nov 13 09:34:19 AM: Solver (including time spent in interface) took 1.469e+01 seconds\n",
      "Number of unique places: 39133\n"
     ]
    }
   ],
   "source": [
    "var = 'd_ha_wt'\n",
    "df1_rh[f'ln_{var}'] = np.log(df1_rh[var])\n",
    "df1_w = tdid.data_filtering_and_weighting(data=df1_rh, control_y=2019, treatment_y=2022, covar='pt_station_num', \n",
    "                                          control_m=[5,], treatment_m=[6, 7, 8], var=f'ln_{var}', unit='h3')\n",
    "df1_w.to_parquet(target_folder + f'h3_grids_9et_{grp}_{lv}_wt_d.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T08:34:52.178217100Z",
     "start_time": "2024-11-13T08:33:58.624705100Z"
    }
   },
   "id": "b4fe03f4630950b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2 The DT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b19578ce77c3af58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.5.3                                    \n",
      "===============================================================================\n",
      "(CVXPY) Nov 13 09:37:00 AM: Your problem has 2126060 variables, 2126063 constraints, and 0 parameters.\n",
      "(CVXPY) Nov 13 09:37:00 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Nov 13 09:37:00 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Nov 13 09:37:00 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "(CVXPY) Nov 13 09:37:00 AM: Your problem is compiled with the CPP canonicalization backend.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:37:00 AM: Compiling problem (target solver=SCS).\n",
      "(CVXPY) Nov 13 09:37:00 AM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> SCS\n",
      "(CVXPY) Nov 13 09:37:00 AM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Nov 13 09:37:00 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Nov 13 09:37:00 AM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Nov 13 09:37:04 AM: Applying reduction SCS\n",
      "(CVXPY) Nov 13 09:37:08 AM: Finished problem compilation (took 8.493e+00 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Nov 13 09:37:08 AM: Invoking solver SCS  to obtain a solution.\n",
      "------------------------------------------------------------------\n",
      "\t       SCS v3.2.7 - Splitting Conic Solver\n",
      "\t(c) Brendan O'Donoghue, Stanford University, 2012\n",
      "------------------------------------------------------------------\n",
      "problem:  variables n: 4252120, constraints m: 8504243\n",
      "cones: \t  z: primal zero / dual free vars: 3\n",
      "\t  l: linear vars: 2126060\n",
      "\t  e: exp vars: 6378180, dual exp vars: 0\n",
      "settings: eps_abs: 1.0e-05, eps_rel: 1.0e-05, eps_infeas: 1.0e-07\n",
      "\t  alpha: 1.50, scale: 1.00e-01, adaptive_scale: 1\n",
      "\t  max_iters: 5000, normalize: 1, rho_x: 1.00e-06\n",
      "\t  acceleration_lookback: 10, acceleration_interval: 10\n",
      "lin-sys:  sparse-direct-amd-qdldl\n",
      "\t  nnz(A): 12744346, nnz(P): 0\n",
      "------------------------------------------------------------------\n",
      " iter | pri res | dua res |   gap   |   obj   |  scale  | time (s)\n",
      "------------------------------------------------------------------\n",
      "     0| 2.29e+01  8.06e-01  1.99e+07 -9.97e+06  1.00e-01  1.04e+01 \n",
      "   250| 4.77e-06  9.88e-08  1.31e-04 -2.00e+00  1.00e-01  1.61e+02 \n",
      "   500| 2.65e-06  1.13e-08  1.62e-04 -2.00e+00  1.00e-01  3.13e+02 \n",
      "   750| 4.62e-06  1.26e-09  1.93e-04 -2.00e+00  1.00e-01  4.63e+02 \n",
      "  1000| 1.33e+01  3.37e-04  1.99e+07 -9.97e+06  1.00e-01  6.16e+02 \n",
      "  1250| 1.23e-06  1.11e-07  1.28e-04 -2.00e+00  7.96e-02  7.67e+02 \n",
      "  1500| 6.67e-07  1.07e-08  1.27e-04 -2.00e+00  7.96e-02  9.15e+02 \n",
      " 1.70e-06  1.24e-09  9.32e-05 -2.00e+00  7.96e-02  1.06e+03 \n",
      " 2.75e-06  1.29e-10  8.13e-05 -2.00e+00  7.96e-02  1.22e+03 \n"
     ]
    }
   ],
   "source": [
    "var = 'num_visits_wt'\n",
    "df2_rh[f'ln_{var}'] = np.log(df2_rh[var])\n",
    "df2_w = tdid.data_filtering_and_weighting(data=df2_rh, control_y=2022, treatment_y=2023, covar='pt_station_num',\n",
    "                                          control_m=[2, 3, 4], treatment_m=[5,], var=f'ln_{var}', unit='h3')\n",
    "df2_w.to_parquet(target_folder + f'h3_grids_dt_{grp}_{lv}_wt_v.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-13T08:36:56.726431400Z"
    }
   },
   "id": "b428396d42432206"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "var = 'd_ha_wt'\n",
    "df2_rh[f'ln_{var}'] = np.log(df2_rh[var])\n",
    "df2_w = tdid.data_filtering_and_weighting(data=df2_rh, control_y=2022, treatment_y=2023, covar='pt_station_num',\n",
    "                                          control_m=[2, 3, 4], treatment_m=[5,], var=f'ln_{var}', unit='h3')\n",
    "df2_w.to_parquet(target_folder + f'h3_grids_dt_{grp}_{lv}_wt_d.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ee1207cbe27e686"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2c59156cc37091c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
